# Team 18 CS470 Final Project
## Transfer Learning from Speaker Verification to Zero-Shot Multispeaker Korean Text-To-Speech Synthesis

Proposal link: https://docs.google.com/document/d/1x8I7riwTYUAlwgf8Ou8jAZ57uNL79e8VIzXJ8rxSlEg/

Korean multispeaker speech dataset: http://www.aihub.or.kr/content/552

## Inference

1. Install required packages by running following command on linux terminal.
```
pip install -r requirements.txt
```

2. Place input text ```input_text.txt``` and reference voice ```input_voice.wav``` in ```./input``` folder.

3. Run ```run.py``` on python interpreter.
```
python3 ./run.py
```

4. Find generated speech at ```./output/generated.wav```.

## Modules in repository

```./input```: Contains user query files (input text / reference voice)

```./output```: Output stored

```./past```: **[unused]** Contains **autoencoder-based, unsupervised speaker embedding generator**. As RNN-based embedder pretrained on English dataset turned out to perform better than this model, we are not using this.

```./tacotron2```: Directory for **tacotron2-based text-to-mel spectrogram synthesizer**. Uses **RNN-based speaker embedding generator** at ```./tacotron2/speaker_embed```, pretrained on English speaker verification task.

```./vocoder```: Directory for **WaveNet-based mel-to-audio vocoder**.

## Overall model description

Our TTS model consists of 3 independently trained modules (location in repo described above).

---
**1. Speaker embedding generator (RNN-based)**
- From several seconds of voice sample in ```.wav``` format, generates 256-dim vector encoding speaker identity (of even unseen ones).
- Trained on English speaker verification task.
- Source: GE2E (https://github.com/CorentinJ/Real-Time-Voice-Cloning)
---
**2. Mel spectrogram synthesizer (Tacotron 2-based)**
- Given input Korean text e.g. ```밤 새기 좋은 날이군.``` and speaker embedding generated by **module 1**, generates mel spectrogram of synthesized speech in voice of reference speaker.
- Trained from scratch by us.
- Source: Tacotron 2 (https://github.com/NVIDIA/tacotron2)
---
**3. Vocoder (WaveNet-based)**
- Given mel spectrogram, reconstructs speech audio in ```.wav``` format.
- Pretrained on English audio, fine-tuned by us.
- Source: WaveRNN (https://github.com/fatchord/WaveRNN)
---
*4. Speaker embedding generator (Speaker2Vec, autoencoder-based) [unused]*
- From several seconds of voice sample in ```.wav``` format, generates 40-dim vector encoding speaker identity (of even unseen ones).
- Trained unsupervisedly assumming short-term active-speaker stationarity (details in report).
- Source: Autoencoder-based architecture implemented by us
- Turned out to be less effective than training **module 1** on English dataset, so not used
---
We constructed a merged pipeline including **module 1, 2 and 3**, so that running ```run.py``` should be sufficient.

## Detailed pipeline
