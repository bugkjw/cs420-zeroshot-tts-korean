# Team 18 CS470 Final Project
20150144 김상우 / 20193138 김재윤 / 20160171 김진우 / 20193649 하진철

## Transfer Learning from Speaker Verification to Zero-Shot Multispeaker Korean Text-To-Speech Synthesis

Project goal: **Option 2** - we are solving our own problem.
Proposal link: https://docs.google.com/document/d/1x8I7riwTYUAlwgf8Ou8jAZ57uNL79e8VIzXJ8rxSlEg/
Korean multispeaker speech dataset: http://www.aihub.or.kr/content/552

## What is this?

We aim to build a TTS system that generates a Korean dialogue from text in the voice of unseen speaker, given only few seconds of speech audio. For this task, we directly applied the model in the following paper: **Ye Jia et al. (2018). Transfer Learning from Speaker Verification to Multispeaker Text-To-Speech Synthesis. ** ***NIPS.***

Yet, instead of just using the complete code, we retrieved separate implementations of submodules of the model and reconstructed the pipeline.

## Module description

Our TTS model consists of 3 independently trained modules (location in repo described above).

---
**1. Speaker embedding generator (RNN-based)**
- From several seconds of voice sample in ```.wav``` format, generates 256-dim vector encoding speaker identity (of even unseen ones).
- Trained on English speaker verification task.
- Source: GE2E (https://github.com/CorentinJ/Real-Time-Voice-Cloning)
---
**2. Mel spectrogram synthesizer (Tacotron 2-based)**
- Given input Korean text e.g. ```밤 새기 좋은 날이군.``` and speaker embedding generated by **module 1**, generates mel spectrogram of synthesized speech in voice of reference speaker.
- Trained from scratch by us.
- Source: Tacotron 2 (https://github.com/NVIDIA/tacotron2)
---
**3. Vocoder (WaveNet-based)**
- Given mel spectrogram, reconstructs speech audio in ```.wav``` format.
- Pretrained on English audio, fine-tuned by us.
- Source: WaveRNN (https://github.com/fatchord/WaveRNN)
---
*4. [unused] Speaker embedding generator (Speaker2Vec, autoencoder-based)*
- From several seconds of voice sample in ```.wav``` format, generates 40-dim vector encoding speaker identity (of even unseen ones).
- Trained unsupervisedly assumming short-term active-speaker stationarity (details in report).
- Source: Autoencoder-based architecture implemented by us
- Turned out to be less effective than training **module 1** on English dataset, so not used
---
We constructed a merged pipeline including **module 1, 2 and 3**, so that running ```run.py``` should be sufficient.

## How to use

Before start, install required packages by running following command on linux terminal.
```
pip install -r requirements.txt
```
##### **Inference**

1. Place input text ```input_text.txt``` and reference voice ```input_voice.wav``` in ```./input``` folder.

2. Run ```run.py``` on python interpreter.
```
python3 ./run.py
```

3. Pre-trained weights and checkpoints are automatically loaded and used for inference.

4. Find generated speech at ```./output/generated.wav```.

##### **Optional: Synthesizer Training**

1. Run ```train_tacotron2.py``` on python interpreter.
```
python3 ./train_tacotron2.py
```

2. Checkpoints are automatically stored (best validation loss model used).

##### **Optional: Vocoder Fine-Tuning**

1. Run ```train_wavernn.py``` on python interpreter.
```
python3 ./train_wavernn.py
```

2. Checkpoints are automatically stored (best validation loss model used).

## Modules in repository

```./input```: Contains user query files (input text / reference voice)

```./output```: Output stored

```./tacotron2```: Directory for **tacotron2-based text-to-mel spectrogram synthesizer**. Uses **RNN-based speaker embedding generator** at ```./tacotron2/speaker_embed```, pretrained on English speaker verification task.

```./vocoder```: Directory for **WaveNet-based mel-to-audio vocoder**.

```./past```: **[unused]** Contains **autoencoder-based, unsupervised speaker embedding generator**. As RNN-based embedder pretrained on English dataset turned out to perform better than this model, we are not using this.

## Repository structure

```
(to be completed)
.
project
│   README.md
│   file001.txt    
│
└───folder1
│   │   file011.txt
│   │   file012.txt
│   │
│   └───subfolder1
│       │   file111.txt
│       │   file112.txt
│       │   ...
│   
└───folder2
    │   file021.txt
    │   file022.txt
```

## Detailed description


